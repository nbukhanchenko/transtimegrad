{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall \"git+https://github.com/nbukhanchenko/transtimegrad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from diffusers import DEISMultistepScheduler\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ttg.model.trans_timegrad import TransTimeGradEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_for_percentile(p):\n",
    "    return (p / 100.0) ** 0.3\n",
    "\n",
    "def plot(\n",
    "    target,\n",
    "    forecast,\n",
    "    prediction_length,\n",
    "    prediction_intervals=(50.0, 90.0),\n",
    "    color=\"g\",\n",
    "    fname=None,\n",
    "):\n",
    "    label_prefix = \"\"\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(24, 24))\n",
    "    axx = axs.ravel()\n",
    "    seq_len, target_dim = target.shape\n",
    "\n",
    "    ps = [50.0] + [\n",
    "        50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, +1.0]\n",
    "    ]\n",
    "\n",
    "    percentiles_sorted = sorted(set(ps))\n",
    "\n",
    "    for dim in range(0, min(rows * cols, target_dim)):\n",
    "        ax = axx[dim]\n",
    "\n",
    "        target[-2 * prediction_length :][dim].plot(ax=ax)\n",
    "\n",
    "        ps_data = [forecast.quantile(p / 100.0)[:, dim] for p in percentiles_sorted]\n",
    "        i_p50 = len(percentiles_sorted) // 2\n",
    "\n",
    "        p50_data = ps_data[i_p50]\n",
    "        p50_series = pd.Series(data=p50_data, index=forecast.index)\n",
    "        p50_series.plot(color=color, ls=\"-\", label=f\"{label_prefix}median\", ax=ax)\n",
    "\n",
    "        for i in range(len(percentiles_sorted) // 2):\n",
    "            ptile = percentiles_sorted[i]\n",
    "            alpha = alpha_for_percentile(ptile)\n",
    "            ax.fill_between(\n",
    "                forecast.index,\n",
    "                ps_data[i],\n",
    "                ps_data[-i - 1],\n",
    "                facecolor=color,\n",
    "                alpha=alpha,\n",
    "                interpolate=True,\n",
    "            )\n",
    "            # Hack to create labels for the error intervals.\n",
    "            # Doesn't actually plot anything, because we only pass a single data point\n",
    "            pd.Series(data=p50_data[:1], index=forecast.index[:1]).plot(\n",
    "                color=color,\n",
    "                alpha=alpha,\n",
    "                linewidth=10,\n",
    "                label=f\"{label_prefix}{100 - ptile * 2}%\",\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "    legend = [\"observations\", \"median prediction\"] + [\n",
    "        f\"{k}% prediction interval\" for k in prediction_intervals\n",
    "    ][::-1]\n",
    "    axx[0].legend(legend, loc=\"upper left\")\n",
    "\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "\n",
    "def prepare_dataset(dataset_name):\n",
    "    dataset = get_dataset(dataset_name, regenerate=False)\n",
    "\n",
    "    train_grouper = MultivariateGrouper(\n",
    "        max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality)\n",
    "    )\n",
    "    test_grouper = MultivariateGrouper(\n",
    "        num_test_dates=int(len(dataset.test) / len(dataset.train)),\n",
    "        max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": train_grouper(dataset.train),\n",
    "        \"test\": test_grouper(dataset.test),\n",
    "        \"metadata\": dataset.metadata\n",
    "    }\n",
    "\n",
    "def prepare_predictor(dataset, max_epochs=256,\n",
    "                      num_train_timesteps=150, beta_start=1e-4, beta_end=0.1, beta_schedule=\"linear\",\n",
    "                      context_length_coef=3, num_layers=2, hidden_size=64, lr=3e-4, weight_decay=1e-8, dropout_rate=0.1,\n",
    "                      lags_seq=[1], num_inference_steps=149, batch_size=64, num_batches_per_epoch=64):\n",
    "    scheduler = DEISMultistepScheduler(\n",
    "        num_train_timesteps=num_train_timesteps,\n",
    "        beta_start=beta_start,\n",
    "        beta_end=beta_end,\n",
    "        beta_schedule=beta_schedule,\n",
    "    )\n",
    "\n",
    "    estimator = TransTimeGradEstimator(\n",
    "        freq=dataset[\"metadata\"].freq,\n",
    "        prediction_length=dataset[\"metadata\"].prediction_length,\n",
    "        input_size=int(dataset[\"metadata\"].feat_static_cat[0].cardinality),\n",
    "        scheduler=scheduler,\n",
    "        context_length=dataset[\"metadata\"].prediction_length * context_length_coef,\n",
    "        num_layers=num_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        dropout_rate=dropout_rate,\n",
    "        scaling=\"mean\",\n",
    "        lags_seq=lags_seq,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        batch_size=batch_size,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "        trainer_kwargs=dict(max_epochs=max_epochs, accelerator=\"gpu\", devices=\"1\"),\n",
    "    )\n",
    "\n",
    "    return estimator.train(dataset[\"train\"], cache_data=True, shuffle_buffer_length=1024)\n",
    "\n",
    "def prepare_metrics(dataset, predictor, num_samples=100):\n",
    "    evaluator = MultivariateEvaluator(\n",
    "        quantiles=(np.arange(20) / 20.0)[1:], target_agg_funcs={\"sum\": np.sum}\n",
    "    )\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset[\"test\"], predictor=predictor, num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    targets = list(ts_it)\n",
    "    agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset[\"test\"]))\n",
    "\n",
    "    return forecasts, targets, agg_metric\n",
    "\n",
    "def prepare_statistics(dataset, forecasts, targets, agg_metric, precision=3):\n",
    "    print(\"CRPS: {}\".format(round(agg_metric[\"mean_wQuantileLoss\"], precision)))\n",
    "    print(\"ND: {}\".format(round(agg_metric[\"ND\"], precision)))\n",
    "    print(\"NRMSE: {}\".format(round(agg_metric[\"NRMSE\"], precision)))\n",
    "    print(\"MSE: {}\".format(round(agg_metric[\"MSE\"], precision)))\n",
    "\n",
    "    print(\"-\" * 32)\n",
    "\n",
    "    print(\"CRPS-Sum: {}\".format(round(agg_metric[\"m_sum_mean_wQuantileLoss\"], precision)))\n",
    "    print(\"ND-Sum: {}\".format(round(agg_metric[\"m_sum_ND\"], precision)))\n",
    "    print(\"NRMSE-Sum: {}\".format(round(agg_metric[\"m_sum_NRMSE\"], precision)))\n",
    "    print(\"MSE-Sum: {}\".format(round(agg_metric[\"m_sum_MSE\"], precision)))\n",
    "\n",
    "    plot(\n",
    "        target=targets[0],\n",
    "        forecast=forecasts[0],\n",
    "        prediction_length=dataset[\"metadata\"].prediction_length,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different types of schedulers\n",
    "# check different trainer_kwargs\n",
    "HYPERPARAMETERS = {\n",
    "    \"max_epochs\":            [128, 256, 512],\n",
    "    \"num_train_timesteps\":   [50, 100, 150, 200, 250],       # explore\n",
    "    \"beta_start\":            [1e-4],\n",
    "    \"beta_end\":              [0.1],\n",
    "    \"beta_schedule\":         [\"linear\"],                     # explore\n",
    "    \"context_length_coef\":   [1, 2, 3, 4, 5],                # explore\n",
    "    \"num_layers\":            [2, 3, 5],                      # explore\n",
    "    \"hidden_size\":           [32, 64, 128],                  # explore\n",
    "    \"lr\":                    [1e-5, 5e-5, 1e-4, 5e-4, 1e-3], # explore\n",
    "    \"weight_decay\":          [1e-9, 1e-8, 1e-7],\n",
    "    \"dropout_rate\":          [0.0, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"lags_seq\":              [None, [1]],                    # explore\n",
    "    \"num_inference_steps\":   [49, 99, 149, 199, 249],\n",
    "    \"batch_size\":            [32, 64, 128],\n",
    "    \"num_batches_per_epoch\": [32, 64, 128],\n",
    "    \"num_samples\":           [50, 100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 186 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 137], [1, 72, 137], [1, 24, 5]] | [1, 100, 24, 137]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.745     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0d2006c4f426f96143559b9105e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41120 (best 0.41120), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.31175 (best 0.31175), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.14226 (best 0.14226), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.08811 (best 0.08811), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.07767 (best 0.07767), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.07093 (best 0.07093), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.06598 (best 0.06598), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.06143 (best 0.06143), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.05992 (best 0.05992), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.05841 (best 0.05841), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.05691 (best 0.05691), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.05547 (best 0.05547), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.05533 (best 0.05533), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.05484 (best 0.05484), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' reached 0.05418 (best 0.05418), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=14-step=960.ckpt' as top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.05330 (best 0.05330), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' reached 0.05304 (best 0.05304), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=16-step=1088.ckpt' as top 1\n",
      "Epoch 17, global step 1152: 'train_loss' reached 0.05207 (best 0.05207), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=17-step=1152.ckpt' as top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.05185 (best 0.05185), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' reached 0.05132 (best 0.05132), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=19-step=1280.ckpt' as top 1\n",
      "Epoch 20, global step 1344: 'train_loss' reached 0.05077 (best 0.05077), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=20-step=1344.ckpt' as top 1\n",
      "Epoch 21, global step 1408: 'train_loss' reached 0.05069 (best 0.05069), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=21-step=1408.ckpt' as top 1\n",
      "Epoch 22, global step 1472: 'train_loss' reached 0.05032 (best 0.05032), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=22-step=1472.ckpt' as top 1\n",
      "Epoch 23, global step 1536: 'train_loss' reached 0.04983 (best 0.04983), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=23-step=1536.ckpt' as top 1\n",
      "Epoch 24, global step 1600: 'train_loss' reached 0.04952 (best 0.04952), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=24-step=1600.ckpt' as top 1\n",
      "Epoch 25, global step 1664: 'train_loss' reached 0.04904 (best 0.04904), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=25-step=1664.ckpt' as top 1\n",
      "Epoch 26, global step 1728: 'train_loss' reached 0.04796 (best 0.04796), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=26-step=1728.ckpt' as top 1\n",
      "Epoch 27, global step 1792: 'train_loss' was not in top 1\n",
      "Epoch 28, global step 1856: 'train_loss' was not in top 1\n",
      "Epoch 29, global step 1920: 'train_loss' was not in top 1\n",
      "Epoch 30, global step 1984: 'train_loss' reached 0.04692 (best 0.04692), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=30-step=1984.ckpt' as top 1\n",
      "Epoch 31, global step 2048: 'train_loss' was not in top 1\n",
      "Epoch 32, global step 2112: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 2176: 'train_loss' was not in top 1\n",
      "Epoch 34, global step 2240: 'train_loss' reached 0.04639 (best 0.04639), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=34-step=2240.ckpt' as top 1\n",
      "Epoch 35, global step 2304: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 2368: 'train_loss' reached 0.04616 (best 0.04616), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=36-step=2368.ckpt' as top 1\n",
      "Epoch 37, global step 2432: 'train_loss' reached 0.04609 (best 0.04609), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=37-step=2432.ckpt' as top 1\n",
      "Epoch 38, global step 2496: 'train_loss' reached 0.04511 (best 0.04511), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=38-step=2496.ckpt' as top 1\n",
      "Epoch 39, global step 2560: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2624: 'train_loss' was not in top 1\n",
      "Epoch 41, global step 2688: 'train_loss' reached 0.04499 (best 0.04499), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=41-step=2688.ckpt' as top 1\n",
      "Epoch 42, global step 2752: 'train_loss' was not in top 1\n",
      "Epoch 43, global step 2816: 'train_loss' was not in top 1\n",
      "Epoch 44, global step 2880: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2944: 'train_loss' reached 0.04478 (best 0.04478), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=45-step=2944.ckpt' as top 1\n",
      "Epoch 46, global step 3008: 'train_loss' reached 0.04426 (best 0.04426), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=46-step=3008.ckpt' as top 1\n",
      "Epoch 47, global step 3072: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 3136: 'train_loss' reached 0.04417 (best 0.04417), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=48-step=3136.ckpt' as top 1\n",
      "Epoch 49, global step 3200: 'train_loss' reached 0.04405 (best 0.04405), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=49-step=3200.ckpt' as top 1\n",
      "Epoch 50, global step 3264: 'train_loss' reached 0.04387 (best 0.04387), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=50-step=3264.ckpt' as top 1\n",
      "Epoch 51, global step 3328: 'train_loss' reached 0.04353 (best 0.04353), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=51-step=3328.ckpt' as top 1\n",
      "Epoch 52, global step 3392: 'train_loss' was not in top 1\n",
      "Epoch 53, global step 3456: 'train_loss' was not in top 1\n",
      "Epoch 54, global step 3520: 'train_loss' reached 0.04304 (best 0.04304), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=54-step=3520.ckpt' as top 1\n",
      "Epoch 55, global step 3584: 'train_loss' was not in top 1\n",
      "Epoch 56, global step 3648: 'train_loss' was not in top 1\n",
      "Epoch 57, global step 3712: 'train_loss' reached 0.04274 (best 0.04274), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=57-step=3712.ckpt' as top 1\n",
      "Epoch 58, global step 3776: 'train_loss' was not in top 1\n",
      "Epoch 59, global step 3840: 'train_loss' was not in top 1\n",
      "Epoch 60, global step 3904: 'train_loss' reached 0.04240 (best 0.04240), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=60-step=3904.ckpt' as top 1\n",
      "Epoch 61, global step 3968: 'train_loss' was not in top 1\n",
      "Epoch 62, global step 4032: 'train_loss' was not in top 1\n",
      "Epoch 63, global step 4096: 'train_loss' was not in top 1\n",
      "Epoch 64, global step 4160: 'train_loss' was not in top 1\n",
      "Epoch 65, global step 4224: 'train_loss' reached 0.04213 (best 0.04213), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=65-step=4224.ckpt' as top 1\n",
      "Epoch 66, global step 4288: 'train_loss' was not in top 1\n",
      "Epoch 67, global step 4352: 'train_loss' reached 0.04212 (best 0.04212), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=67-step=4352.ckpt' as top 1\n",
      "Epoch 68, global step 4416: 'train_loss' was not in top 1\n",
      "Epoch 69, global step 4480: 'train_loss' was not in top 1\n",
      "Epoch 70, global step 4544: 'train_loss' was not in top 1\n",
      "Epoch 71, global step 4608: 'train_loss' was not in top 1\n",
      "Epoch 72, global step 4672: 'train_loss' was not in top 1\n",
      "Epoch 73, global step 4736: 'train_loss' reached 0.04143 (best 0.04143), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=73-step=4736.ckpt' as top 1\n",
      "Epoch 74, global step 4800: 'train_loss' was not in top 1\n",
      "Epoch 75, global step 4864: 'train_loss' was not in top 1\n",
      "Epoch 76, global step 4928: 'train_loss' was not in top 1\n",
      "Epoch 77, global step 4992: 'train_loss' was not in top 1\n",
      "Epoch 78, global step 5056: 'train_loss' was not in top 1\n",
      "Epoch 79, global step 5120: 'train_loss' was not in top 1\n",
      "Epoch 80, global step 5184: 'train_loss' reached 0.04105 (best 0.04105), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=80-step=5184.ckpt' as top 1\n",
      "Epoch 81, global step 5248: 'train_loss' was not in top 1\n",
      "Epoch 82, global step 5312: 'train_loss' was not in top 1\n",
      "Epoch 83, global step 5376: 'train_loss' reached 0.04102 (best 0.04102), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=83-step=5376.ckpt' as top 1\n",
      "Epoch 84, global step 5440: 'train_loss' was not in top 1\n",
      "Epoch 85, global step 5504: 'train_loss' was not in top 1\n",
      "Epoch 86, global step 5568: 'train_loss' was not in top 1\n",
      "Epoch 87, global step 5632: 'train_loss' was not in top 1\n",
      "Epoch 88, global step 5696: 'train_loss' was not in top 1\n",
      "Epoch 89, global step 5760: 'train_loss' was not in top 1\n",
      "Epoch 90, global step 5824: 'train_loss' was not in top 1\n",
      "Epoch 91, global step 5888: 'train_loss' was not in top 1\n",
      "Epoch 92, global step 5952: 'train_loss' reached 0.04096 (best 0.04096), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=92-step=5952.ckpt' as top 1\n",
      "Epoch 93, global step 6016: 'train_loss' was not in top 1\n",
      "Epoch 94, global step 6080: 'train_loss' reached 0.04093 (best 0.04093), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=94-step=6080.ckpt' as top 1\n",
      "Epoch 95, global step 6144: 'train_loss' was not in top 1\n",
      "Epoch 96, global step 6208: 'train_loss' reached 0.04055 (best 0.04055), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=96-step=6208.ckpt' as top 1\n",
      "Epoch 97, global step 6272: 'train_loss' was not in top 1\n",
      "Epoch 98, global step 6336: 'train_loss' was not in top 1\n",
      "Epoch 99, global step 6400: 'train_loss' was not in top 1\n",
      "Epoch 100, global step 6464: 'train_loss' reached 0.04048 (best 0.04048), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=100-step=6464.ckpt' as top 1\n",
      "Epoch 101, global step 6528: 'train_loss' reached 0.03971 (best 0.03971), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=101-step=6528.ckpt' as top 1\n",
      "Epoch 102, global step 6592: 'train_loss' was not in top 1\n",
      "Epoch 103, global step 6656: 'train_loss' was not in top 1\n",
      "Epoch 104, global step 6720: 'train_loss' was not in top 1\n",
      "Epoch 105, global step 6784: 'train_loss' was not in top 1\n",
      "Epoch 106, global step 6848: 'train_loss' was not in top 1\n",
      "Epoch 107, global step 6912: 'train_loss' was not in top 1\n",
      "Epoch 108, global step 6976: 'train_loss' was not in top 1\n",
      "Epoch 109, global step 7040: 'train_loss' was not in top 1\n",
      "Epoch 110, global step 7104: 'train_loss' was not in top 1\n",
      "Epoch 111, global step 7168: 'train_loss' was not in top 1\n",
      "Epoch 112, global step 7232: 'train_loss' was not in top 1\n",
      "Epoch 113, global step 7296: 'train_loss' was not in top 1\n",
      "Epoch 114, global step 7360: 'train_loss' reached 0.03966 (best 0.03966), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=114-step=7360.ckpt' as top 1\n",
      "Epoch 115, global step 7424: 'train_loss' was not in top 1\n",
      "Epoch 116, global step 7488: 'train_loss' was not in top 1\n",
      "Epoch 117, global step 7552: 'train_loss' was not in top 1\n",
      "Epoch 118, global step 7616: 'train_loss' was not in top 1\n",
      "Epoch 119, global step 7680: 'train_loss' was not in top 1\n",
      "Epoch 120, global step 7744: 'train_loss' was not in top 1\n",
      "Epoch 121, global step 7808: 'train_loss' was not in top 1\n",
      "Epoch 122, global step 7872: 'train_loss' was not in top 1\n",
      "Epoch 123, global step 7936: 'train_loss' was not in top 1\n",
      "Epoch 124, global step 8000: 'train_loss' was not in top 1\n",
      "Epoch 125, global step 8064: 'train_loss' was not in top 1\n",
      "Epoch 126, global step 8128: 'train_loss' reached 0.03900 (best 0.03900), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=126-step=8128.ckpt' as top 1\n",
      "Epoch 127, global step 8192: 'train_loss' was not in top 1\n",
      "Epoch 128, global step 8256: 'train_loss' was not in top 1\n",
      "Epoch 129, global step 8320: 'train_loss' was not in top 1\n",
      "Epoch 130, global step 8384: 'train_loss' was not in top 1\n",
      "Epoch 131, global step 8448: 'train_loss' was not in top 1\n",
      "Epoch 132, global step 8512: 'train_loss' was not in top 1\n",
      "Epoch 133, global step 8576: 'train_loss' was not in top 1\n",
      "Epoch 134, global step 8640: 'train_loss' was not in top 1\n",
      "Epoch 135, global step 8704: 'train_loss' was not in top 1\n",
      "Epoch 136, global step 8768: 'train_loss' was not in top 1\n",
      "Epoch 137, global step 8832: 'train_loss' was not in top 1\n",
      "Epoch 138, global step 8896: 'train_loss' was not in top 1\n",
      "Epoch 139, global step 8960: 'train_loss' was not in top 1\n",
      "Epoch 140, global step 9024: 'train_loss' was not in top 1\n",
      "Epoch 141, global step 9088: 'train_loss' was not in top 1\n",
      "Epoch 142, global step 9152: 'train_loss' was not in top 1\n",
      "Epoch 143, global step 9216: 'train_loss' was not in top 1\n",
      "Epoch 144, global step 9280: 'train_loss' was not in top 1\n",
      "Epoch 145, global step 9344: 'train_loss' was not in top 1\n",
      "Epoch 146, global step 9408: 'train_loss' was not in top 1\n",
      "Epoch 147, global step 9472: 'train_loss' was not in top 1\n",
      "Epoch 148, global step 9536: 'train_loss' was not in top 1\n",
      "Epoch 149, global step 9600: 'train_loss' was not in top 1\n",
      "Epoch 150, global step 9664: 'train_loss' was not in top 1\n",
      "Epoch 151, global step 9728: 'train_loss' was not in top 1\n",
      "Epoch 152, global step 9792: 'train_loss' was not in top 1\n",
      "Epoch 153, global step 9856: 'train_loss' was not in top 1\n",
      "Epoch 154, global step 9920: 'train_loss' was not in top 1\n",
      "Epoch 155, global step 9984: 'train_loss' was not in top 1\n",
      "Epoch 156, global step 10048: 'train_loss' was not in top 1\n",
      "Epoch 157, global step 10112: 'train_loss' was not in top 1\n",
      "Epoch 158, global step 10176: 'train_loss' was not in top 1\n",
      "Epoch 159, global step 10240: 'train_loss' was not in top 1\n",
      "Epoch 160, global step 10304: 'train_loss' was not in top 1\n",
      "Epoch 161, global step 10368: 'train_loss' was not in top 1\n",
      "Epoch 162, global step 10432: 'train_loss' was not in top 1\n",
      "Epoch 163, global step 10496: 'train_loss' was not in top 1\n",
      "Epoch 164, global step 10560: 'train_loss' was not in top 1\n",
      "Epoch 165, global step 10624: 'train_loss' was not in top 1\n",
      "Epoch 166, global step 10688: 'train_loss' was not in top 1\n",
      "Epoch 167, global step 10752: 'train_loss' was not in top 1\n",
      "Epoch 168, global step 10816: 'train_loss' was not in top 1\n",
      "Epoch 169, global step 10880: 'train_loss' was not in top 1\n",
      "Epoch 170, global step 10944: 'train_loss' was not in top 1\n",
      "Epoch 171, global step 11008: 'train_loss' was not in top 1\n",
      "Epoch 172, global step 11072: 'train_loss' was not in top 1\n",
      "Epoch 173, global step 11136: 'train_loss' was not in top 1\n",
      "Epoch 174, global step 11200: 'train_loss' was not in top 1\n",
      "Epoch 175, global step 11264: 'train_loss' was not in top 1\n",
      "Epoch 176, global step 11328: 'train_loss' was not in top 1\n",
      "Epoch 177, global step 11392: 'train_loss' was not in top 1\n",
      "Epoch 178, global step 11456: 'train_loss' was not in top 1\n",
      "Epoch 179, global step 11520: 'train_loss' was not in top 1\n",
      "Epoch 180, global step 11584: 'train_loss' was not in top 1\n",
      "Epoch 181, global step 11648: 'train_loss' was not in top 1\n",
      "Epoch 182, global step 11712: 'train_loss' was not in top 1\n",
      "Epoch 183, global step 11776: 'train_loss' was not in top 1\n",
      "Epoch 184, global step 11840: 'train_loss' was not in top 1\n",
      "Epoch 185, global step 11904: 'train_loss' was not in top 1\n",
      "Epoch 186, global step 11968: 'train_loss' was not in top 1\n",
      "Epoch 187, global step 12032: 'train_loss' was not in top 1\n",
      "Epoch 188, global step 12096: 'train_loss' was not in top 1\n",
      "Epoch 189, global step 12160: 'train_loss' was not in top 1\n",
      "Epoch 190, global step 12224: 'train_loss' was not in top 1\n",
      "Epoch 191, global step 12288: 'train_loss' was not in top 1\n",
      "Epoch 192, global step 12352: 'train_loss' was not in top 1\n",
      "Epoch 193, global step 12416: 'train_loss' was not in top 1\n",
      "Epoch 194, global step 12480: 'train_loss' was not in top 1\n",
      "Epoch 195, global step 12544: 'train_loss' was not in top 1\n",
      "Epoch 196, global step 12608: 'train_loss' was not in top 1\n",
      "Epoch 197, global step 12672: 'train_loss' was not in top 1\n",
      "Epoch 198, global step 12736: 'train_loss' was not in top 1\n",
      "Epoch 199, global step 12800: 'train_loss' was not in top 1\n",
      "Epoch 200, global step 12864: 'train_loss' was not in top 1\n",
      "Epoch 201, global step 12928: 'train_loss' was not in top 1\n",
      "Epoch 202, global step 12992: 'train_loss' was not in top 1\n",
      "Epoch 203, global step 13056: 'train_loss' was not in top 1\n",
      "Epoch 204, global step 13120: 'train_loss' was not in top 1\n",
      "Epoch 205, global step 13184: 'train_loss' was not in top 1\n",
      "Epoch 206, global step 13248: 'train_loss' was not in top 1\n",
      "Epoch 207, global step 13312: 'train_loss' was not in top 1\n",
      "Epoch 208, global step 13376: 'train_loss' was not in top 1\n",
      "Epoch 209, global step 13440: 'train_loss' reached 0.03887 (best 0.03887), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=209-step=13440.ckpt' as top 1\n",
      "Epoch 210, global step 13504: 'train_loss' was not in top 1\n",
      "Epoch 211, global step 13568: 'train_loss' was not in top 1\n",
      "Epoch 212, global step 13632: 'train_loss' was not in top 1\n",
      "Epoch 213, global step 13696: 'train_loss' was not in top 1\n",
      "Epoch 214, global step 13760: 'train_loss' was not in top 1\n",
      "Epoch 215, global step 13824: 'train_loss' was not in top 1\n",
      "Epoch 216, global step 13888: 'train_loss' was not in top 1\n",
      "Epoch 217, global step 13952: 'train_loss' was not in top 1\n",
      "Epoch 218, global step 14016: 'train_loss' was not in top 1\n",
      "Epoch 219, global step 14080: 'train_loss' was not in top 1\n",
      "Epoch 220, global step 14144: 'train_loss' was not in top 1\n",
      "Epoch 221, global step 14208: 'train_loss' was not in top 1\n",
      "Epoch 222, global step 14272: 'train_loss' was not in top 1\n",
      "Epoch 223, global step 14336: 'train_loss' was not in top 1\n",
      "Epoch 224, global step 14400: 'train_loss' was not in top 1\n",
      "Epoch 225, global step 14464: 'train_loss' was not in top 1\n",
      "Epoch 226, global step 14528: 'train_loss' was not in top 1\n",
      "Epoch 227, global step 14592: 'train_loss' was not in top 1\n",
      "Epoch 228, global step 14656: 'train_loss' was not in top 1\n",
      "Epoch 229, global step 14720: 'train_loss' was not in top 1\n",
      "Epoch 230, global step 14784: 'train_loss' was not in top 1\n",
      "Epoch 231, global step 14848: 'train_loss' was not in top 1\n",
      "Epoch 232, global step 14912: 'train_loss' was not in top 1\n",
      "Epoch 233, global step 14976: 'train_loss' was not in top 1\n",
      "Epoch 234, global step 15040: 'train_loss' was not in top 1\n",
      "Epoch 235, global step 15104: 'train_loss' was not in top 1\n",
      "Epoch 236, global step 15168: 'train_loss' was not in top 1\n",
      "Epoch 237, global step 15232: 'train_loss' was not in top 1\n",
      "Epoch 238, global step 15296: 'train_loss' was not in top 1\n",
      "Epoch 239, global step 15360: 'train_loss' was not in top 1\n",
      "Epoch 240, global step 15424: 'train_loss' was not in top 1\n",
      "Epoch 241, global step 15488: 'train_loss' was not in top 1\n",
      "Epoch 242, global step 15552: 'train_loss' was not in top 1\n",
      "Epoch 243, global step 15616: 'train_loss' was not in top 1\n",
      "Epoch 244, global step 15680: 'train_loss' was not in top 1\n",
      "Epoch 245, global step 15744: 'train_loss' was not in top 1\n",
      "Epoch 246, global step 15808: 'train_loss' was not in top 1\n",
      "Epoch 247, global step 15872: 'train_loss' was not in top 1\n",
      "Epoch 248, global step 15936: 'train_loss' was not in top 1\n",
      "Epoch 249, global step 16000: 'train_loss' was not in top 1\n",
      "Epoch 250, global step 16064: 'train_loss' was not in top 1\n",
      "Epoch 251, global step 16128: 'train_loss' was not in top 1\n",
      "Epoch 252, global step 16192: 'train_loss' was not in top 1\n",
      "Epoch 253, global step 16256: 'train_loss' was not in top 1\n",
      "Epoch 254, global step 16320: 'train_loss' was not in top 1\n",
      "Epoch 255, global step 16384: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=256` reached.\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 247.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 195.84it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.84it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 221.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 271.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 228.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 253.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.94it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 214.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 250.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 251.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 262.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.36it/s]\n",
      "\n",
      "Running evaluation: 0it [00:00, ?it/s]\u001b[A/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "Running evaluation: 7it [00:00, 162.57it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return pd.Period(val, freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 432 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 370], [1, 72, 370], [1, 24, 5]] | [1, 100, 24, 370]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "432 K     Trainable params\n",
      "0         Non-trainable params\n",
      "432 K     Total params\n",
      "1.729     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286588046ec4d25bd3e71198a3a4013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41410 (best 0.41410), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.30522 (best 0.30522), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.11737 (best 0.11737), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.07334 (best 0.07334), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.06130 (best 0.06130), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.05290 (best 0.05290), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.04940 (best 0.04940), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.04809 (best 0.04809), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.04619 (best 0.04619), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.04516 (best 0.04516), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.04366 (best 0.04366), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.04333 (best 0.04333), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.04259 (best 0.04259), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.04198 (best 0.04198), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' reached 0.04136 (best 0.04136), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=14-step=960.ckpt' as top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.04046 (best 0.04046), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 1152: 'train_loss' reached 0.03960 (best 0.03960), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=17-step=1152.ckpt' as top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.03911 (best 0.03911), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' reached 0.03910 (best 0.03910), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=19-step=1280.ckpt' as top 1\n",
      "Epoch 20, global step 1344: 'train_loss' reached 0.03806 (best 0.03806), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=20-step=1344.ckpt' as top 1\n",
      "Epoch 21, global step 1408: 'train_loss' was not in top 1\n",
      "Epoch 22, global step 1472: 'train_loss' was not in top 1\n",
      "Epoch 23, global step 1536: 'train_loss' reached 0.03762 (best 0.03762), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=23-step=1536.ckpt' as top 1\n",
      "Epoch 24, global step 1600: 'train_loss' reached 0.03760 (best 0.03760), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=24-step=1600.ckpt' as top 1\n",
      "Epoch 25, global step 1664: 'train_loss' reached 0.03748 (best 0.03748), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=25-step=1664.ckpt' as top 1\n",
      "Epoch 26, global step 1728: 'train_loss' was not in top 1\n",
      "Epoch 27, global step 1792: 'train_loss' reached 0.03686 (best 0.03686), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=27-step=1792.ckpt' as top 1\n",
      "Epoch 28, global step 1856: 'train_loss' reached 0.03670 (best 0.03670), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=28-step=1856.ckpt' as top 1\n",
      "Epoch 29, global step 1920: 'train_loss' reached 0.03661 (best 0.03661), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=29-step=1920.ckpt' as top 1\n",
      "Epoch 30, global step 1984: 'train_loss' reached 0.03654 (best 0.03654), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=30-step=1984.ckpt' as top 1\n",
      "Epoch 31, global step 2048: 'train_loss' reached 0.03557 (best 0.03557), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=31-step=2048.ckpt' as top 1\n",
      "Epoch 32, global step 2112: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 2176: 'train_loss' was not in top 1\n",
      "Epoch 34, global step 2240: 'train_loss' reached 0.03520 (best 0.03520), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=34-step=2240.ckpt' as top 1\n",
      "Epoch 35, global step 2304: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 2368: 'train_loss' was not in top 1\n",
      "Epoch 37, global step 2432: 'train_loss' was not in top 1\n",
      "Epoch 38, global step 2496: 'train_loss' reached 0.03463 (best 0.03463), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=38-step=2496.ckpt' as top 1\n",
      "Epoch 39, global step 2560: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2624: 'train_loss' reached 0.03457 (best 0.03457), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=40-step=2624.ckpt' as top 1\n",
      "Epoch 41, global step 2688: 'train_loss' was not in top 1\n",
      "Epoch 42, global step 2752: 'train_loss' reached 0.03402 (best 0.03402), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=42-step=2752.ckpt' as top 1\n",
      "Epoch 43, global step 2816: 'train_loss' reached 0.03392 (best 0.03392), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=43-step=2816.ckpt' as top 1\n",
      "Epoch 44, global step 2880: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2944: 'train_loss' reached 0.03387 (best 0.03387), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=45-step=2944.ckpt' as top 1\n",
      "Epoch 46, global step 3008: 'train_loss' reached 0.03373 (best 0.03373), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=46-step=3008.ckpt' as top 1\n",
      "Epoch 47, global step 3072: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 3136: 'train_loss' was not in top 1\n",
      "Epoch 49, global step 3200: 'train_loss' reached 0.03358 (best 0.03358), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=49-step=3200.ckpt' as top 1\n",
      "Epoch 50, global step 3264: 'train_loss' was not in top 1\n",
      "Epoch 51, global step 3328: 'train_loss' was not in top 1\n",
      "Epoch 52, global step 3392: 'train_loss' reached 0.03343 (best 0.03343), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=52-step=3392.ckpt' as top 1\n",
      "Epoch 53, global step 3456: 'train_loss' reached 0.03321 (best 0.03321), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=53-step=3456.ckpt' as top 1\n",
      "Epoch 54, global step 3520: 'train_loss' was not in top 1\n",
      "Epoch 55, global step 3584: 'train_loss' reached 0.03284 (best 0.03284), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=55-step=3584.ckpt' as top 1\n",
      "Epoch 56, global step 3648: 'train_loss' was not in top 1\n",
      "Epoch 57, global step 3712: 'train_loss' was not in top 1\n",
      "Epoch 58, global step 3776: 'train_loss' was not in top 1\n",
      "Epoch 59, global step 3840: 'train_loss' was not in top 1\n",
      "Epoch 60, global step 3904: 'train_loss' reached 0.03235 (best 0.03235), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=60-step=3904.ckpt' as top 1\n",
      "Epoch 61, global step 3968: 'train_loss' was not in top 1\n",
      "Epoch 62, global step 4032: 'train_loss' was not in top 1\n",
      "Epoch 63, global step 4096: 'train_loss' was not in top 1\n",
      "Epoch 64, global step 4160: 'train_loss' was not in top 1\n",
      "Epoch 65, global step 4224: 'train_loss' reached 0.03180 (best 0.03180), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=65-step=4224.ckpt' as top 1\n",
      "Epoch 66, global step 4288: 'train_loss' was not in top 1\n",
      "Epoch 67, global step 4352: 'train_loss' was not in top 1\n",
      "Epoch 68, global step 4416: 'train_loss' reached 0.03140 (best 0.03140), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=68-step=4416.ckpt' as top 1\n",
      "Epoch 69, global step 4480: 'train_loss' was not in top 1\n",
      "Epoch 70, global step 4544: 'train_loss' was not in top 1\n",
      "Epoch 71, global step 4608: 'train_loss' was not in top 1\n",
      "Epoch 72, global step 4672: 'train_loss' was not in top 1\n",
      "Epoch 73, global step 4736: 'train_loss' was not in top 1\n",
      "Epoch 74, global step 4800: 'train_loss' was not in top 1\n",
      "Epoch 75, global step 4864: 'train_loss' was not in top 1\n",
      "Epoch 76, global step 4928: 'train_loss' was not in top 1\n",
      "Epoch 77, global step 4992: 'train_loss' reached 0.03118 (best 0.03118), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=77-step=4992.ckpt' as top 1\n",
      "Epoch 78, global step 5056: 'train_loss' was not in top 1\n",
      "Epoch 79, global step 5120: 'train_loss' was not in top 1\n",
      "Epoch 80, global step 5184: 'train_loss' was not in top 1\n",
      "Epoch 81, global step 5248: 'train_loss' was not in top 1\n",
      "Epoch 82, global step 5312: 'train_loss' was not in top 1\n",
      "Epoch 83, global step 5376: 'train_loss' reached 0.03100 (best 0.03100), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=83-step=5376.ckpt' as top 1\n",
      "Epoch 84, global step 5440: 'train_loss' was not in top 1\n",
      "Epoch 85, global step 5504: 'train_loss' was not in top 1\n",
      "Epoch 86, global step 5568: 'train_loss' was not in top 1\n",
      "Epoch 87, global step 5632: 'train_loss' reached 0.03077 (best 0.03077), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=87-step=5632.ckpt' as top 1\n",
      "Epoch 88, global step 5696: 'train_loss' was not in top 1\n",
      "Epoch 89, global step 5760: 'train_loss' reached 0.03021 (best 0.03021), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=89-step=5760.ckpt' as top 1\n",
      "Epoch 90, global step 5824: 'train_loss' was not in top 1\n",
      "Epoch 91, global step 5888: 'train_loss' was not in top 1\n",
      "Epoch 92, global step 5952: 'train_loss' was not in top 1\n",
      "Epoch 93, global step 6016: 'train_loss' was not in top 1\n",
      "Epoch 94, global step 6080: 'train_loss' was not in top 1\n",
      "Epoch 95, global step 6144: 'train_loss' was not in top 1\n",
      "Epoch 96, global step 6208: 'train_loss' was not in top 1\n",
      "Epoch 97, global step 6272: 'train_loss' was not in top 1\n",
      "Epoch 98, global step 6336: 'train_loss' was not in top 1\n",
      "Epoch 99, global step 6400: 'train_loss' was not in top 1\n",
      "Epoch 100, global step 6464: 'train_loss' was not in top 1\n",
      "Epoch 101, global step 6528: 'train_loss' reached 0.02993 (best 0.02993), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=101-step=6528.ckpt' as top 1\n",
      "Epoch 102, global step 6592: 'train_loss' was not in top 1\n",
      "Epoch 103, global step 6656: 'train_loss' was not in top 1\n",
      "Epoch 104, global step 6720: 'train_loss' was not in top 1\n",
      "Epoch 105, global step 6784: 'train_loss' was not in top 1\n",
      "Epoch 106, global step 6848: 'train_loss' reached 0.02981 (best 0.02981), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=106-step=6848.ckpt' as top 1\n",
      "Epoch 107, global step 6912: 'train_loss' reached 0.02974 (best 0.02974), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=107-step=6912.ckpt' as top 1\n",
      "Epoch 108, global step 6976: 'train_loss' was not in top 1\n",
      "Epoch 109, global step 7040: 'train_loss' reached 0.02956 (best 0.02956), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=109-step=7040.ckpt' as top 1\n",
      "Epoch 110, global step 7104: 'train_loss' was not in top 1\n",
      "Epoch 111, global step 7168: 'train_loss' was not in top 1\n",
      "Epoch 112, global step 7232: 'train_loss' was not in top 1\n",
      "Epoch 113, global step 7296: 'train_loss' was not in top 1\n",
      "Epoch 114, global step 7360: 'train_loss' reached 0.02955 (best 0.02955), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=114-step=7360.ckpt' as top 1\n",
      "Epoch 115, global step 7424: 'train_loss' was not in top 1\n",
      "Epoch 116, global step 7488: 'train_loss' was not in top 1\n",
      "Epoch 117, global step 7552: 'train_loss' was not in top 1\n",
      "Epoch 118, global step 7616: 'train_loss' was not in top 1\n",
      "Epoch 119, global step 7680: 'train_loss' was not in top 1\n",
      "Epoch 120, global step 7744: 'train_loss' reached 0.02950 (best 0.02950), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=120-step=7744.ckpt' as top 1\n",
      "Epoch 121, global step 7808: 'train_loss' reached 0.02947 (best 0.02947), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=121-step=7808.ckpt' as top 1\n",
      "Epoch 122, global step 7872: 'train_loss' reached 0.02931 (best 0.02931), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=122-step=7872.ckpt' as top 1\n",
      "Epoch 123, global step 7936: 'train_loss' was not in top 1\n",
      "Epoch 124, global step 8000: 'train_loss' was not in top 1\n",
      "Epoch 125, global step 8064: 'train_loss' was not in top 1\n",
      "Epoch 126, global step 8128: 'train_loss' was not in top 1\n",
      "Epoch 127, global step 8192: 'train_loss' was not in top 1\n",
      "Epoch 128, global step 8256: 'train_loss' was not in top 1\n",
      "Epoch 129, global step 8320: 'train_loss' was not in top 1\n",
      "Epoch 130, global step 8384: 'train_loss' was not in top 1\n",
      "Epoch 131, global step 8448: 'train_loss' reached 0.02909 (best 0.02909), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=131-step=8448.ckpt' as top 1\n",
      "Epoch 132, global step 8512: 'train_loss' was not in top 1\n",
      "Epoch 133, global step 8576: 'train_loss' was not in top 1\n",
      "Epoch 134, global step 8640: 'train_loss' was not in top 1\n",
      "Epoch 135, global step 8704: 'train_loss' was not in top 1\n",
      "Epoch 136, global step 8768: 'train_loss' was not in top 1\n",
      "Epoch 137, global step 8832: 'train_loss' was not in top 1\n",
      "Epoch 138, global step 8896: 'train_loss' was not in top 1\n",
      "Epoch 139, global step 8960: 'train_loss' was not in top 1\n",
      "Epoch 140, global step 9024: 'train_loss' was not in top 1\n",
      "Epoch 141, global step 9088: 'train_loss' was not in top 1\n",
      "Epoch 142, global step 9152: 'train_loss' was not in top 1\n",
      "Epoch 143, global step 9216: 'train_loss' was not in top 1\n",
      "Epoch 144, global step 9280: 'train_loss' reached 0.02892 (best 0.02892), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=144-step=9280.ckpt' as top 1\n",
      "Epoch 145, global step 9344: 'train_loss' was not in top 1\n",
      "Epoch 146, global step 9408: 'train_loss' reached 0.02847 (best 0.02847), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=146-step=9408.ckpt' as top 1\n",
      "Epoch 147, global step 9472: 'train_loss' was not in top 1\n",
      "Epoch 148, global step 9536: 'train_loss' was not in top 1\n",
      "Epoch 149, global step 9600: 'train_loss' was not in top 1\n",
      "Epoch 150, global step 9664: 'train_loss' was not in top 1\n",
      "Epoch 151, global step 9728: 'train_loss' was not in top 1\n",
      "Epoch 152, global step 9792: 'train_loss' was not in top 1\n",
      "Epoch 153, global step 9856: 'train_loss' was not in top 1\n",
      "Epoch 154, global step 9920: 'train_loss' was not in top 1\n",
      "Epoch 155, global step 9984: 'train_loss' was not in top 1\n",
      "Epoch 156, global step 10048: 'train_loss' was not in top 1\n",
      "Epoch 157, global step 10112: 'train_loss' was not in top 1\n",
      "Epoch 158, global step 10176: 'train_loss' was not in top 1\n",
      "Epoch 159, global step 10240: 'train_loss' reached 0.02831 (best 0.02831), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=159-step=10240.ckpt' as top 1\n",
      "Epoch 160, global step 10304: 'train_loss' was not in top 1\n",
      "Epoch 161, global step 10368: 'train_loss' was not in top 1\n",
      "Epoch 162, global step 10432: 'train_loss' was not in top 1\n",
      "Epoch 163, global step 10496: 'train_loss' was not in top 1\n",
      "Epoch 164, global step 10560: 'train_loss' was not in top 1\n",
      "Epoch 165, global step 10624: 'train_loss' was not in top 1\n",
      "Epoch 166, global step 10688: 'train_loss' was not in top 1\n",
      "Epoch 167, global step 10752: 'train_loss' was not in top 1\n",
      "Epoch 168, global step 10816: 'train_loss' was not in top 1\n",
      "Epoch 169, global step 10880: 'train_loss' was not in top 1\n",
      "Epoch 170, global step 10944: 'train_loss' was not in top 1\n",
      "Epoch 171, global step 11008: 'train_loss' was not in top 1\n",
      "Epoch 172, global step 11072: 'train_loss' was not in top 1\n",
      "Epoch 173, global step 11136: 'train_loss' was not in top 1\n",
      "Epoch 174, global step 11200: 'train_loss' was not in top 1\n",
      "Epoch 175, global step 11264: 'train_loss' was not in top 1\n",
      "Epoch 176, global step 11328: 'train_loss' was not in top 1\n",
      "Epoch 177, global step 11392: 'train_loss' was not in top 1\n",
      "Epoch 178, global step 11456: 'train_loss' was not in top 1\n",
      "Epoch 179, global step 11520: 'train_loss' was not in top 1\n",
      "Epoch 180, global step 11584: 'train_loss' was not in top 1\n",
      "Epoch 181, global step 11648: 'train_loss' was not in top 1\n",
      "Epoch 182, global step 11712: 'train_loss' was not in top 1\n",
      "Epoch 183, global step 11776: 'train_loss' was not in top 1\n",
      "Epoch 184, global step 11840: 'train_loss' was not in top 1\n",
      "Epoch 185, global step 11904: 'train_loss' was not in top 1\n",
      "Epoch 186, global step 11968: 'train_loss' was not in top 1\n",
      "Epoch 187, global step 12032: 'train_loss' was not in top 1\n",
      "Epoch 188, global step 12096: 'train_loss' was not in top 1\n",
      "Epoch 189, global step 12160: 'train_loss' was not in top 1\n",
      "Epoch 190, global step 12224: 'train_loss' was not in top 1\n",
      "Epoch 191, global step 12288: 'train_loss' was not in top 1\n",
      "Epoch 192, global step 12352: 'train_loss' was not in top 1\n",
      "Epoch 193, global step 12416: 'train_loss' was not in top 1\n",
      "Epoch 194, global step 12480: 'train_loss' was not in top 1\n",
      "Epoch 195, global step 12544: 'train_loss' was not in top 1\n",
      "Epoch 196, global step 12608: 'train_loss' was not in top 1\n",
      "Epoch 197, global step 12672: 'train_loss' was not in top 1\n",
      "Epoch 198, global step 12736: 'train_loss' was not in top 1\n",
      "Epoch 199, global step 12800: 'train_loss' was not in top 1\n",
      "Epoch 200, global step 12864: 'train_loss' was not in top 1\n",
      "Epoch 201, global step 12928: 'train_loss' was not in top 1\n",
      "Epoch 202, global step 12992: 'train_loss' was not in top 1\n",
      "Epoch 203, global step 13056: 'train_loss' was not in top 1\n",
      "Epoch 204, global step 13120: 'train_loss' was not in top 1\n",
      "Epoch 205, global step 13184: 'train_loss' was not in top 1\n",
      "Epoch 206, global step 13248: 'train_loss' was not in top 1\n",
      "Epoch 207, global step 13312: 'train_loss' was not in top 1\n",
      "Epoch 208, global step 13376: 'train_loss' was not in top 1\n",
      "Epoch 209, global step 13440: 'train_loss' was not in top 1\n",
      "Epoch 210, global step 13504: 'train_loss' was not in top 1\n",
      "Epoch 211, global step 13568: 'train_loss' was not in top 1\n",
      "Epoch 212, global step 13632: 'train_loss' was not in top 1\n",
      "Epoch 213, global step 13696: 'train_loss' was not in top 1\n",
      "Epoch 214, global step 13760: 'train_loss' reached 0.02822 (best 0.02822), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=214-step=13760.ckpt' as top 1\n",
      "Epoch 215, global step 13824: 'train_loss' was not in top 1\n",
      "Epoch 216, global step 13888: 'train_loss' was not in top 1\n",
      "Epoch 217, global step 13952: 'train_loss' was not in top 1\n",
      "Epoch 218, global step 14016: 'train_loss' was not in top 1\n",
      "Epoch 219, global step 14080: 'train_loss' was not in top 1\n",
      "Epoch 220, global step 14144: 'train_loss' was not in top 1\n",
      "Epoch 221, global step 14208: 'train_loss' was not in top 1\n",
      "Epoch 222, global step 14272: 'train_loss' was not in top 1\n",
      "Epoch 223, global step 14336: 'train_loss' was not in top 1\n",
      "Epoch 224, global step 14400: 'train_loss' was not in top 1\n",
      "Epoch 225, global step 14464: 'train_loss' was not in top 1\n",
      "Epoch 226, global step 14528: 'train_loss' was not in top 1\n",
      "Epoch 227, global step 14592: 'train_loss' was not in top 1\n",
      "Epoch 228, global step 14656: 'train_loss' was not in top 1\n",
      "Epoch 229, global step 14720: 'train_loss' was not in top 1\n",
      "Epoch 230, global step 14784: 'train_loss' was not in top 1\n",
      "Epoch 231, global step 14848: 'train_loss' was not in top 1\n",
      "Epoch 232, global step 14912: 'train_loss' was not in top 1\n",
      "Epoch 233, global step 14976: 'train_loss' was not in top 1\n",
      "Epoch 234, global step 15040: 'train_loss' was not in top 1\n",
      "Epoch 235, global step 15104: 'train_loss' reached 0.02794 (best 0.02794), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=235-step=15104.ckpt' as top 1\n",
      "Epoch 236, global step 15168: 'train_loss' was not in top 1\n",
      "Epoch 237, global step 15232: 'train_loss' was not in top 1\n",
      "Epoch 238, global step 15296: 'train_loss' was not in top 1\n",
      "Epoch 239, global step 15360: 'train_loss' was not in top 1\n",
      "Epoch 240, global step 15424: 'train_loss' was not in top 1\n",
      "Epoch 241, global step 15488: 'train_loss' was not in top 1\n",
      "Epoch 242, global step 15552: 'train_loss' was not in top 1\n",
      "Epoch 243, global step 15616: 'train_loss' was not in top 1\n",
      "Epoch 244, global step 15680: 'train_loss' was not in top 1\n",
      "Epoch 245, global step 15744: 'train_loss' was not in top 1\n",
      "Epoch 246, global step 15808: 'train_loss' was not in top 1\n",
      "Epoch 247, global step 15872: 'train_loss' was not in top 1\n",
      "Epoch 248, global step 15936: 'train_loss' was not in top 1\n",
      "Epoch 249, global step 16000: 'train_loss' was not in top 1\n",
      "Epoch 250, global step 16064: 'train_loss' was not in top 1\n",
      "Epoch 251, global step 16128: 'train_loss' was not in top 1\n",
      "Epoch 252, global step 16192: 'train_loss' was not in top 1\n",
      "Epoch 253, global step 16256: 'train_loss' was not in top 1\n",
      "Epoch 254, global step 16320: 'train_loss' was not in top 1\n",
      "Epoch 255, global step 16384: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=256` reached.\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 242.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.97it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 296.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 208.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.23it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 198.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 227.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 253.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 208.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.97it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 258.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 266.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 299.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.77it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 254.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 220.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 114.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 226.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 220.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 214.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.77it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 152.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 44.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.56it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 250.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 273.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.43it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 249.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.94it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 251.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 272.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 221.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 269.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.24it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 265.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 248.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.43it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.23it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 247.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 261.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 254.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 246.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 255.31it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 266.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 199.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.68it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: UserWarning: Warning: converting a masked element to nan.\n",
      "  return arr.astype(dtype, copy=True)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:531: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[\"NRMSE\"] = totals[\"RMSE\"] / totals[\"abs_target_mean\"]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:532: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[\"ND\"] = totals[\"abs_error\"] / totals[\"abs_target_sum\"]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:536: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[f\"QuantileLoss[{quantile}]\"] / totals[\"abs_target_sum\"]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 205.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 178.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 270.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 296.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 199.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 249.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 198.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 262.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.24it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 242.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 272.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 196.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.71it/s]\n",
      "\n",
      "Running evaluation: 0it [00:00, ?it/s]\u001b[A/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "Running evaluation: 7it [00:00, 165.74it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  return pd.Period(val, freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:113: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  timestamp + len(data[FieldName.TARGET]) - 1,\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:242: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index=pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:242: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index=pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:187: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:187: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:198: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:198: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                         | Out sizes      \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 73.4 K | [[1, 1], [1, 1], [1, 90, 4], [1, 90, 8], [1, 90, 8], [1, 30, 4]] | [1, 100, 30, 8]\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "73.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "73.4 K    Total params\n",
      "0.294     Total estimated model params size (MB)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/feature.py:364: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index = pd.period_range(start, periods=length, freq=start.freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/feature.py:364: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index = pd.period_range(start, periods=length, freq=start.freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/split.py:150: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  entry[self.start_field] + idx + self.lead_time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1092ffb7814a5683adca0f8f9211c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41521 (best 0.41521), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.28120 (best 0.28120), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.08373 (best 0.08373), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.04662 (best 0.04662), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.03065 (best 0.03065), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.02148 (best 0.02148), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.01611 (best 0.01611), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.01328 (best 0.01328), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.01189 (best 0.01189), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.01063 (best 0.01063), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.01004 (best 0.01004), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.00924 (best 0.00924), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.00840 (best 0.00840), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.00801 (best 0.00801), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' was not in top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.00759 (best 0.00759), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 1152: 'train_loss' was not in top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.00706 (best 0.00706), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' was not in top 1\n",
      "Epoch 20, global step 1344: 'train_loss' was not in top 1\n",
      "Epoch 21, global step 1408: 'train_loss' reached 0.00657 (best 0.00657), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=21-step=1408.ckpt' as top 1\n",
      "Epoch 22, global step 1472: 'train_loss' reached 0.00642 (best 0.00642), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=22-step=1472.ckpt' as top 1\n",
      "Epoch 23, global step 1536: 'train_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"solar_nips\": [],\n",
    "    \"electricity_nips\": [],\n",
    "    \"exchange_rate_nips\": []\n",
    "}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset = prepare_dataset(dataset_name)\n",
    "    predictor = prepare_predictor(dataset)\n",
    "    forecasts, targets, agg_metric = prepare_metrics(dataset, predictor)\n",
    "    datasets[dataset_name] = {\n",
    "        \"dataset\": dataset,\n",
    "        \"predictor\": predictor,\n",
    "        \"forecasts\": forecasts,\n",
    "        \"targets\": targets,\n",
    "        \"agg_metric\": agg_metric\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"solar_nips\"][\"dataset\"], datasets[\"solar_nips\"][\"forecasts\"],\n",
    "    datasets[\"solar_nips\"][\"targets\"], datasets[\"solar_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"electricity_nips\"][\"dataset\"], datasets[\"electricity_nips\"][\"forecasts\"],\n",
    "    datasets[\"electricity_nips\"][\"targets\"], datasets[\"electricity_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"exchange_rate_nips\"][\"dataset\"], datasets[\"exchange_rate_nips\"][\"forecasts\"],\n",
    "    datasets[\"exchange_rate_nips\"][\"targets\"], datasets[\"exchange_rate_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
