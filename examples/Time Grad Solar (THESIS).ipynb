{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/artificial/_base.py:84: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.freq: BaseOffset = to_offset(freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "from diffusers import (\n",
    "    PNDMScheduler,\n",
    "    DDIMScheduler,\n",
    "    DDPMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    KDPM2DiscreteScheduler,\n",
    "    DEISMultistepScheduler,\n",
    ")\n",
    "\n",
    "# from pts.model.time_grad import TimeGradEstimator\n",
    "\n",
    "import os\n",
    "\n",
    "LIB_PATH = \"~/Desktop/THESIS/transtimegrad\"\n",
    "os.chdir(os.path.expanduser(LIB_PATH))\n",
    "from ttg.model.rnn_timegrad import TimeGradEstimator\n",
    "from ttg.dataset.repository.datasets import dataset_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepeare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\"solar_nips\", regenerate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaData(freq='H', target=None, feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='137')], feat_static_real=[], feat_dynamic_real=[], feat_dynamic_cat=[], prediction_length=24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouper = MultivariateGrouper(\n",
    "    max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality)\n",
    ")\n",
    "\n",
    "test_grouper = MultivariateGrouper(\n",
    "    num_test_dates=int(len(dataset.test) / len(dataset.train)),\n",
    "    max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return pd.Period(val, freq)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = train_grouper(dataset.train)\n",
    "dataset_test = test_grouper(dataset.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    target,\n",
    "    forecast,\n",
    "    prediction_length,\n",
    "    prediction_intervals=(50.0, 90.0),\n",
    "    color=\"g\",\n",
    "    fname=None,\n",
    "):\n",
    "    label_prefix = \"\"\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(24, 24))\n",
    "    axx = axs.ravel()\n",
    "    seq_len, target_dim = target.shape\n",
    "\n",
    "    ps = [50.0] + [\n",
    "        50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, +1.0]\n",
    "    ]\n",
    "\n",
    "    percentiles_sorted = sorted(set(ps))\n",
    "\n",
    "    def alpha_for_percentile(p):\n",
    "        return (p / 100.0) ** 0.3\n",
    "\n",
    "    for dim in range(0, min(rows * cols, target_dim)):\n",
    "        ax = axx[dim]\n",
    "\n",
    "        target[-2 * prediction_length :][dim].plot(ax=ax)\n",
    "\n",
    "        ps_data = [forecast.quantile(p / 100.0)[:, dim] for p in percentiles_sorted]\n",
    "        i_p50 = len(percentiles_sorted) // 2\n",
    "\n",
    "        p50_data = ps_data[i_p50]\n",
    "        p50_series = pd.Series(data=p50_data, index=forecast.index)\n",
    "        p50_series.plot(color=color, ls=\"-\", label=f\"{label_prefix}median\", ax=ax)\n",
    "\n",
    "        for i in range(len(percentiles_sorted) // 2):\n",
    "            ptile = percentiles_sorted[i]\n",
    "            alpha = alpha_for_percentile(ptile)\n",
    "            ax.fill_between(\n",
    "                forecast.index,\n",
    "                ps_data[i],\n",
    "                ps_data[-i - 1],\n",
    "                facecolor=color,\n",
    "                alpha=alpha,\n",
    "                interpolate=True,\n",
    "            )\n",
    "            # Hack to create labels for the error intervals.\n",
    "            # Doesn't actually plot anything, because we only pass a single data point\n",
    "            pd.Series(data=p50_data[:1], index=forecast.index[:1]).plot(\n",
    "                color=color,\n",
    "                alpha=alpha,\n",
    "                linewidth=10,\n",
    "                label=f\"{label_prefix}{100 - ptile * 2}%\",\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "    legend = [\"observations\", \"median prediction\"] + [\n",
    "        f\"{k}% prediction interval\" for k in prediction_intervals\n",
    "    ][::-1]\n",
    "    axx[0].legend(legend, loc=\"upper left\")\n",
    "\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches=\"tight\", pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(\n",
    "    quantiles=(np.arange(20) / 20.0)[1:], target_agg_funcs={\"sum\": np.sum}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TimeGrad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PNDMScheduler(\n",
    "    num_train_timesteps=100,\n",
    "    beta_end=0.2,\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=100,\n",
    "    beta_end=0.1,\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    "    thresholding=True,\n",
    "    clip_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DEISMultistepScheduler(\n",
    "    num_train_timesteps=150,\n",
    "    beta_end=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n"
     ]
    }
   ],
   "source": [
    "estimator = TimeGradEstimator(\n",
    "    input_size=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout_rate=0.1,\n",
    "    lags_seq=[1],\n",
    "    scheduler=scheduler,\n",
    "    num_inference_steps=149,\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=dataset.metadata.prediction_length * 3,\n",
    "    freq=dataset.metadata.freq,\n",
    "    scaling=\"mean\",\n",
    "    trainer_kwargs=dict(max_epochs=200, accelerator=\"gpu\", devices=\"1\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Missing logger folder: /Users/npbukhanchenko/Desktop/THESIS/transtimegrad/lightning_logs\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 186 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 137], [1, 72, 137], [1, 24, 5]] | [1, 100, 24, 137]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.745     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4017ad079934963ad7be2a7ca0902ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 0.32997 (best 0.32997), saving model to '/Users/npbukhanchenko/Desktop/THESIS/transtimegrad/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(dataset_train, cache_data=True, shuffle_buffer_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset_test, predictor=predictor, num_samples=100\n",
    ")\n",
    "forecasts = list(forecast_it)\n",
    "targets = list(ts_it)\n",
    "agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRPS: {}\".format(agg_metric[\"mean_wQuantileLoss\"]))\n",
    "print(\"ND: {}\".format(agg_metric[\"ND\"]))\n",
    "print(\"NRMSE: {}\".format(agg_metric[\"NRMSE\"]))\n",
    "print(\"MSE: {}\".format(agg_metric[\"MSE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRPS-Sum: {}\".format(agg_metric[\"m_sum_mean_wQuantileLoss\"]))\n",
    "print(\"ND-Sum: {}\".format(agg_metric[\"m_sum_ND\"]))\n",
    "print(\"NRMSE-Sum: {}\".format(agg_metric[\"m_sum_NRMSE\"]))\n",
    "print(\"MSE-Sum: {}\".format(agg_metric[\"m_sum_MSE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    target=targets[0],\n",
    "    forecast=forecasts[0],\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
